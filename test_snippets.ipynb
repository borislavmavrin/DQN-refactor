{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "done_mask = tf.placeholder(tf.bool, shape=None)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(\n",
    "    done_mask,\n",
    "    feed_dict={\n",
    "        done_mask: 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "state = tf.placeholder(tf.float32, shape=[None, 80, 80, 4])\n",
    "scope = 'q'\n",
    "reuse=False\n",
    "num_actions = 5\n",
    "\n",
    "def add_linear(state, scope, reuse):\n",
    "    _, img_height, img_width, nchannels = state.shape\n",
    "    state_flat_dim = img_height * img_width * nchannels\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        batch_state = tf.reshape(state, [-1, state_flat_dim])\n",
    "        W = tf.get_variable(\n",
    "            \"weights\",\n",
    "            shape=(state_flat_dim, num_actions),\n",
    "            initializer=tf.constant_initializer(1.)\n",
    "        )\n",
    "        tf.add_to_collection(scope, W)\n",
    "        b = tf.get_variable(\n",
    "            \"bias\",\n",
    "            shape=(num_actions),\n",
    "            initializer=tf.constant_initializer(0.)\n",
    "        )\n",
    "        tf.add_to_collection(scope, b)\n",
    "        out = tf.matmul(batch_state, W) + b\n",
    "    return out\n",
    "\n",
    "\n",
    "out = add_linear(state, scope, reuse=False)\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "output_np = sess.run(\n",
    "    out,\n",
    "    feed_dict={\n",
    "        state: np.ones((2, 80, 80, 4))\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "output_np == np.array([\n",
    "    [25600., 25600., 25600., 25600., 25600.],\n",
    "    [25600., 25600., 25600., 25600., 25600.]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"target_q\"):\n",
    "    for i in range(2):\n",
    "        W = tf.get_variable(\n",
    "            \"weights_\" + str(i),\n",
    "            shape=(2, 2),\n",
    "            initializer=tf.constant_initializer(1.)\n",
    "        )\n",
    "        tf.add_to_collection('target_q', W)\n",
    "\n",
    "with tf.variable_scope(\"q\"):\n",
    "    for i in range(2):\n",
    "        W = tf.get_variable(\n",
    "            \"weights_\" + str(i),\n",
    "            shape=(2, 2),\n",
    "            initializer=tf.constant_initializer(2.)\n",
    "        )\n",
    "        tf.add_to_collection('q', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'q/weights_0:0' shape=(2, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'q/weights_1:0' shape=(2, 2) dtype=float32_ref>]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_var_lst = tf.get_collection('target_q')\n",
    "q_var_lst = tf.get_collection('q')\n",
    "update_op_lst = []\n",
    "for idx, target_q_var in enumerate(target_q_var_lst):\n",
    "    op = target_q_var.assign(q_var_lst[idx])\n",
    "    update_op_lst.append(op.op)\n",
    "update_op_grouped = tf.group(*update_op_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 1.],\n",
      "       [1., 1.]], dtype=float32), array([[1., 1.],\n",
      "       [1., 1.]], dtype=float32)]\n",
      "[array([[2., 2.],\n",
      "       [2., 2.]], dtype=float32), array([[2., 2.],\n",
      "       [2., 2.]], dtype=float32)]\n",
      "[array([[2., 2.],\n",
      "       [2., 2.]], dtype=float32), array([[2., 2.],\n",
      "       [2., 2.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(sess.run(\n",
    "    target_q_var_lst,\n",
    "))\n",
    "\n",
    "sess.run(update_op_grouped)\n",
    "\n",
    "print(sess.run(\n",
    "    target_q_var_lst,\n",
    "))\n",
    "\n",
    "print(sess.run(\n",
    "    q_var_lst,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([0, 1])\n",
    "q = tf.constant([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.6, 0.5, 0.4]   \n",
    "])\n",
    "target_q = tf.constant([\n",
    "    [1.1, 1.2, 1.3],\n",
    "    [1.6, 1.5, 1.4]   \n",
    "])\n",
    "r = tf.constant([1., 10.])\n",
    "done_mask = tf.constant([0., 1.])\n",
    "num_actions = 3\n",
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_one_hot = tf.one_hot(a, depth=num_actions)\n",
    "loss_agg = r + \\\n",
    "(1. - done_mask) * gamma * tf.reduce_max(target_q, axis=1) - \\\n",
    "tf.reduce_sum(tf.multiply(q, a_one_hot), axis=1)\n",
    "loss = tf.reduce_mean(loss_agg ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.545"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 1])\n",
    "q = np.array([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.6, 0.5, 0.4]   \n",
    "])\n",
    "target_q = np.array([\n",
    "    [1.1, 1.2, 1.3],\n",
    "    [1.6, 1.5, 1.4]   \n",
    "])\n",
    "r = np.array([1., 10.])\n",
    "done_mask = np.array([0., 1.])\n",
    "num_actions = 3\n",
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_agg = r + (1. - done_mask) * target_q.max(axis=1) - q[[0, 1], a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.545"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_agg ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "s = tf.placeholder(tf.float32, shape=[None, 2, 2, 1])\n",
    "sp = tf.placeholder(tf.float32, shape=[None, 2, 2, 1])\n",
    "a = tf.placeholder(tf.int32, shape=None)\n",
    "r = tf.placeholder(tf.float32, shape=None)\n",
    "gamma = tf.placeholder(tf.float32, shape=())\n",
    "done_mask = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "num_actions = 2\n",
    "\n",
    "def add_linear(state, scope, reuse, init_value):\n",
    "    _, img_height, img_width, nchannels = state.shape\n",
    "    state_flat_dim = img_height * img_width * nchannels\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        batch_state = tf.reshape(state, [-1, state_flat_dim])\n",
    "        W = tf.get_variable(\n",
    "            \"weights\",\n",
    "            shape=(state_flat_dim, num_actions),\n",
    "            initializer=tf.constant_initializer(init_value)\n",
    "        )\n",
    "        tf.add_to_collection(scope, W)\n",
    "        b = tf.get_variable(\n",
    "            \"bias\",\n",
    "            shape=(num_actions),\n",
    "            initializer=tf.constant_initializer(init_value)\n",
    "        )\n",
    "        tf.add_to_collection(scope, b)\n",
    "        out = tf.matmul(batch_state, W) + b\n",
    "    return out\n",
    "\n",
    "target_q = add_linear(sp, scope='target_q', reuse=False, init_value=1.)\n",
    "q = add_linear(s, scope='q', reuse=False, init_value=2.)\n",
    "a_one_hot = tf.one_hot(a, depth=num_actions)\n",
    "\n",
    "loss_agg = r + \\\n",
    "  (1. - done_mask) * gamma * tf.reduce_max(target_q, axis=1) - \\\n",
    "    tf.reduce_sum(tf.multiply(q, a_one_hot), axis=1)\n",
    "loss = tf.reduce_mean(loss_agg ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = 'q'\n",
    "lr = 0.01\n",
    "grad_clip = True\n",
    "clip_val = 1.\n",
    "var_lst = tf.get_collection(scope)\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "grads_and_vars_lst = optimizer.compute_gradients(\n",
    "    loss, var_list=var_lst)\n",
    "\n",
    "# self.train_op = optimizer.minimize(self.loss, var_list=var_lst)\n",
    "\n",
    "if grad_clip:\n",
    "    grads_clipped_and_vars_lst = []\n",
    "    for grad_and_var in grads_and_vars_lst:\n",
    "        grad, var = grad_and_var\n",
    "        grad_clipped = tf.clip_by_norm(grad, clip_val)\n",
    "        grads_clipped_and_vars_lst.append((grad_clipped, var))\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_clipped_and_vars_lst)\n",
    "    grads_lst = [x[0] for x in grads_clipped_and_vars_lst]\n",
    "else:\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars_lst)\n",
    "    grads_lst = [x[0] for x in grads_and_vars_lst]\n",
    "\n",
    "# global norm is just a norm of stack vectors\n",
    "grad_norm = tf.global_norm(grads_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(\n",
    "    loss,\n",
    "    feed_dict={\n",
    "        s: np.ones((1, 2, 2, 1)),\n",
    "        a: np.array([1]),\n",
    "        r: np.array([0.]),\n",
    "        sp: np.ones((1, 2, 2, 1)),\n",
    "        done_mask: np.array([1.]),\n",
    "        gamma: 1.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 87.79691, 1.4142135]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(\n",
    "    [train_op,\n",
    "     loss,\n",
    "     grad_norm\n",
    "     \n",
    "    ],\n",
    "    feed_dict={\n",
    "        s: np.ones((1, 2, 2, 1)),\n",
    "        a: np.array([0]),\n",
    "        r: np.array([0.]),\n",
    "        sp: np.ones((1, 2, 2, 1)),\n",
    "        done_mask: np.array([1.]),\n",
    "        gamma: 1.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vars_lst = tf.get_collection('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.0008222 , 0.00751421],\n",
       "        [0.0008222 , 0.00751421],\n",
       "        [0.0008222 , 0.00751421],\n",
       "        [0.0008222 , 0.00751421]], dtype=float32),\n",
       " array([0.0008222 , 0.00751421], dtype=float32)]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(q_vars_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1., 1.])\n",
    "sess.run(tf.norm([a], ord=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer op CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "s = tf.placeholder(tf.float32, shape=[None, 80, 80, 1])\n",
    "sp = tf.placeholder(tf.float32, shape=[None, 80, 80, 1])\n",
    "a = tf.placeholder(tf.int32, shape=None)\n",
    "r = tf.placeholder(tf.float32, shape=None)\n",
    "gamma = tf.placeholder(tf.float32, shape=())\n",
    "done_mask = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "num_actions = 2\n",
    "\n",
    "def add_CNN(state, scope, reuse, init_value):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        layer1 = tf.contrib.layers.conv2d(\n",
    "            inputs=state,\n",
    "            num_outputs=32,\n",
    "            kernel_size=[8, 8],\n",
    "            stride=4,\n",
    "            padding=\"SAME\",\n",
    "            activation_fn=tf.nn.relu,\n",
    "            variables_collections=scope\n",
    "        )\n",
    "\n",
    "        layer2 = tf.contrib.layers.conv2d(\n",
    "            inputs=layer1,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[4, 4],\n",
    "            stride=2,\n",
    "            padding=\"SAME\",\n",
    "            activation_fn=tf.nn.relu,\n",
    "            variables_collections=scope\n",
    "        )\n",
    "\n",
    "        layer3 = tf.contrib.layers.conv2d(\n",
    "            inputs=layer2,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[3, 3],\n",
    "            stride=1,\n",
    "            padding=\"SAME\",\n",
    "            activation_fn=tf.nn.relu,\n",
    "            variables_collections=scope\n",
    "        )\n",
    "\n",
    "        layer4 = tf.contrib.layers.fully_connected(\n",
    "            tf.reshape(layer3, [-1, 10 * 10 * 64]),\n",
    "            512,\n",
    "            activation_fn=tf.nn.relu,\n",
    "            variables_collections=scope\n",
    "        )\n",
    "\n",
    "        layer5 = tf.contrib.layers.fully_connected(\n",
    "            layer4,\n",
    "            num_actions,\n",
    "            activation_fn=None,\n",
    "            variables_collections=scope\n",
    "        )\n",
    "        out = layer5\n",
    "    return out\n",
    "\n",
    "\n",
    "target_q = add_CNN(sp, scope='target_q', reuse=False, init_value=1.)\n",
    "q = add_CNN(s, scope='q', reuse=False, init_value=2.)\n",
    "a_one_hot = tf.one_hot(a, depth=num_actions)\n",
    "\n",
    "loss_agg = r + \\\n",
    "  (1. - done_mask) * gamma * tf.reduce_max(target_q, axis=1) - \\\n",
    "    tf.reduce_sum(tf.multiply(q, a_one_hot), axis=1)\n",
    "loss = tf.reduce_mean(loss_agg ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = 'q'\n",
    "lr = 0.05\n",
    "grad_clip = True\n",
    "clip_val = 10.\n",
    "var_lst = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'q/Conv/weights:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'q/Conv/biases:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'q/Conv_1/weights:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'q/Conv_1/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'q/Conv_2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'q/Conv_2/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'q/fully_connected/weights:0' shape=(6400, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'q/fully_connected/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'q/fully_connected_1/weights:0' shape=(512, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'q/fully_connected_1/biases:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "grads_and_vars_lst = optimizer.compute_gradients(\n",
    "    loss, var_list=var_lst)\n",
    "\n",
    "# self.train_op = optimizer.minimize(self.loss, var_list=var_lst)\n",
    "\n",
    "if grad_clip:\n",
    "    grads_clipped_and_vars_lst = []\n",
    "    for grad_and_var in grads_and_vars_lst:\n",
    "        grad, var = grad_and_var\n",
    "        grad_clipped = tf.clip_by_norm(grad, clip_val)\n",
    "        grads_clipped_and_vars_lst.append((grad_clipped, var))\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_clipped_and_vars_lst)\n",
    "    grads_lst = [x[0] for x in grads_clipped_and_vars_lst]\n",
    "else:\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars_lst)\n",
    "    grads_lst = [x[0] for x in grads_and_vars_lst]\n",
    "\n",
    "# global norm is just a norm of stack vectors\n",
    "grad_norm = tf.global_norm(grads_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163891200.0"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(\n",
    "    loss,\n",
    "    feed_dict={\n",
    "        s: np.ones((1, 80, 80, 1)),\n",
    "        a: np.array([1]),\n",
    "        r: np.array([0.]),\n",
    "        sp: np.ones((1, 80, 80, 1)),\n",
    "        done_mask: np.array([1.]),\n",
    "        gamma: 1.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 23.794497, 13.970612]"
      ]
     },
     "execution_count": 954,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(\n",
    "    [train_op,\n",
    "     loss,\n",
    "     grad_norm\n",
    "     \n",
    "    ],\n",
    "    feed_dict={\n",
    "        s: np.ones((1, 80, 80, 1)),\n",
    "        a: np.array([0]),\n",
    "        r: np.array([0.]),\n",
    "        sp: np.ones((1, 80, 80, 1)),\n",
    "        done_mask: np.array([1.]),\n",
    "        gamma: 1.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vars_lst = tf.get_collection('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.0008222 , 0.00751421],\n",
       "        [0.0008222 , 0.00751421],\n",
       "        [0.0008222 , 0.00751421],\n",
       "        [0.0008222 , 0.00751421]], dtype=float32),\n",
       " array([0.0008222 , 0.00751421], dtype=float32)]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(q_vars_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1., 1.])\n",
    "sess.run(tf.norm([a], ord=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
